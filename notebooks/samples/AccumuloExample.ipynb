{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook assumes you have an Accumulo cluster running and have a shaded jar file with accumulo dependencies.\n",
    "- To setup a simple Accumulo cluster: https://github.com/apache/fluo-uno\n",
    "- To build a shaded jar with accumulo dependencies: https://github.com/apache/accumulo-examples/tree/master/spark\n",
    "\n",
    "Adding the shaded jar after the notebook is running does not work, it needs to be added prior to starting jupyter\n",
    "\n",
    "```\n",
    "SPARK_OPTS=\"--jars ~/repos/accumulo-examples/spark/target/accumulo-spark-shaded.jar\" jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:///home/scgraham/repos/mmlspark/target/scala-2.11/mmlspark_2.11-0.17-159-6b4d8bfb-20190912-0312-SNAPSHOT.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished download of mmlspark_2.11-0.17-159-6b4d8bfb-20190912-0312-SNAPSHOT.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Adding the MMLSpark JAR dynamically does work\n",
    "%AddJar file:///home/scgraham/repos/mmlspark/target/scala-2.11/mmlspark_2.11-0.17-159-6b4d8bfb-20190912-0312-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.apache.spark:spark-avro_2.11:2.4.0 for download\n",
      "Obtained 2 files\n"
     ]
    }
   ],
   "source": [
    "%AddDeps org.apache.spark spark-avro_2.11 2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.util.Properties\n",
    "import scala.collection.JavaConverters._\n",
    "import scala.collection.JavaConversions._\n",
    "\n",
    "import org.apache.accumulo.core.client.{Accumulo, AccumuloClient, BatchWriter}\n",
    "import org.apache.accumulo.core.data.{Key, Mutation, Value}\n",
    "import org.apache.accumulo.core.security.Authorizations\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.FileSystem\n",
    "import org.apache.hadoop.fs.Path\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.{DataType, MetadataBuilder, StringType, StructType, StructField}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Accumulo Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputTable = spark_example_input\n",
       "outputTable = spark_example_output\n",
       "rootPath = /spark_example\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/spark_example"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputTable = \"spark_example_input\"\n",
    "val outputTable = \"spark_example_output\"\n",
    "val rootPath = new Path(\"/spark_example/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accumuloClientProperties: String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def accumuloClientProperties = \"/home/scgraham/repos/fluo-uno/install/accumulo-2.0.0/conf/accumulo-client.properties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "props = {auth.type=password, auth.principal=root, instance.zookeepers=localhost:2181, instance.name=uno, auth.token=secret}\n",
       "client = org.apache.accumulo.core.clientImpl.ClientContext@2d439cab\n",
       "hdfs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_104873185_46, ugi=scgraham (auth:SIMPLE)]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_104873185_46, ugi=scgraham (auth:SIMPLE)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create accumulo client\n",
    "val props = Accumulo.newClientProperties().from(accumuloClientProperties).build();\n",
    "val client = Accumulo.newClient().from(props).build()\n",
    "\n",
    "val hdfs = FileSystem.get(new Configuration());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// cleanup hdfs root path\n",
    "if (hdfs.exists(rootPath)) {\n",
    "  hdfs.delete(rootPath, true);\n",
    "}\n",
    "\n",
    "// reset accumulo tables\n",
    "Seq(inputTable, outputTable).foreach(table => {\n",
    "    if (client.tableOperations().exists(table)) {\n",
    "      client.tableOperations().delete(table)\n",
    "    }\n",
    "    client.tableOperations().create(table)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splits = [0025, 0050, 0075]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0025, 0050, 0075]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.TreeSet\n",
    "import org.apache.hadoop.io.Text\n",
    "val splits = new TreeSet[Text]()\n",
    "splits.add(new Text(\"0025\"))\n",
    "splits.add(new Text(\"0050\"))\n",
    "splits.add(new Text(\"0075\"))\n",
    "\n",
    "client.tableOperations().addSplits(inputTable, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batchWriter = org.apache.accumulo.core.clientImpl.BatchWriterImpl@12198766\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.accumulo.core.clientImpl.BatchWriterImpl@12198766"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// write data to input table\n",
    "val batchWriter = client.createBatchWriter(inputTable)\n",
    "for (i <- 0 until 100) {\n",
    "  val m = new Mutation(f\"$i%03d\")\n",
    "  m.at().family(\"cf1\").qualifier(\"cq1\").put(\"row_\" + i);\n",
    "  batchWriter.addMutation(m);\n",
    "}\n",
    "batchWriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scanner = org.apache.accumulo.core.clientImpl.ScannerImpl@6657b0e6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.accumulo.core.clientImpl.ScannerImpl@6657b0e6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scanner = client.createScanner(inputTable, Authorizations.EMPTY)\n",
    "scanner.fetchColumnFamily(\"cf1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 cf1:cq1 [] 1568258494560 false=row_0\n",
      "001 cf1:cq1 [] 1568258494560 false=row_1\n",
      "002 cf1:cq1 [] 1568258494560 false=row_2\n",
      "003 cf1:cq1 [] 1568258494560 false=row_3\n",
      "004 cf1:cq1 [] 1568258494560 false=row_4\n",
      "005 cf1:cq1 [] 1568258494560 false=row_5\n",
      "006 cf1:cq1 [] 1568258494560 false=row_6\n",
      "007 cf1:cq1 [] 1568258494560 false=row_7\n",
      "008 cf1:cq1 [] 1568258494560 false=row_8\n",
      "009 cf1:cq1 [] 1568258494560 false=row_9\n"
     ]
    }
   ],
   "source": [
    "scanner.zipWithIndex.foreach { case(e, i) => if (i < 10) println(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark - Accumulo Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.SparkConf@31faa778\n",
       "spark = org.apache.spark.sql.SparkSession@155fb1e7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@155fb1e7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// reload Spark session with config\n",
    "\n",
    "// KryoSerializer is needed for serializing Accumulo Key when partitioning data for bulk import\n",
    "val conf = new SparkConf()\n",
    "    .setAppName(\"AccumuloDataSourceExample\")\n",
    "    .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .registerKryoClasses(Array(classOf[Key], classOf[Value], classOf[Properties]))\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .config(conf)\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "properties = Map(auth.type -> password, maxPartitions -> 200, auth.principal -> root, table -> spark_example_input, instance.zookeepers -> localhost:2181, instance.name -> uno, auth.token -> secret)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(auth.type -> password, maxPartitions -> 200, auth.principal -> root, table -> spark_example_input, instance.zookeepers -> localhost:2181, instance.name -> uno, auth.token -> secret)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props.put(\"table\", inputTable)\n",
    "props.put(\"maxPartitions\", \"200\")\n",
    "val properties = props.asScala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(cf1,StructType(StructField(cq1,StringType,true)),false))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(cf1,StructType(StructField(cq1,StringType,true)),false))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = new StructType().add(\"cf1\", new StructType().add(\"cq1\", StringType, true), false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    cf1|\n",
      "+-------+\n",
      "|[row_0]|\n",
      "|[row_1]|\n",
      "|[row_2]|\n",
      "|[row_3]|\n",
      "|[row_4]|\n",
      "|[row_5]|\n",
      "|[row_6]|\n",
      "|[row_7]|\n",
      "|[row_8]|\n",
      "|[row_9]|\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [cf1: struct<cq1: string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[cf1: struct<cq1: string>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "    .read\n",
    "    .format(\"com.microsoft.ml.spark.accumulo\")\n",
    "    .options(properties)\n",
    "    .schema(schema)\n",
    "    .load()\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark_example_input"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props.put(\"table\", outputTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Writing job aborted.\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:92)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:260)\n",
       "  ... 50 elided\n",
       "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 15, scgraham-centos, executor 1): java.lang.IllegalStateException: Can not add to mutation after serializing it\n",
       "\tat org.apache.accumulo.core.data.Mutation$Options.put(Mutation.java:1260)\n",
       "\tat org.apache.accumulo.core.data.Mutation$Options.put(Mutation.java:1346)\n",
       "\tat org.apache.accumulo.core.data.Mutation$Options.put(Mutation.java:1335)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloDataWriter$$anonfun$write$1$$anonfun$apply$1.apply(AccumuloDataWriter.scala:36)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloDataWriter$$anonfun$write$1$$anonfun$apply$1.apply(AccumuloDataWriter.scala:30)\n",
       "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloDataWriter$$anonfun$write$1.apply(AccumuloDataWriter.scala:30)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloDataWriter$$anonfun$write$1.apply(AccumuloDataWriter.scala:28)\n",
       "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloDataWriter.write(AccumuloDataWriter.scala:28)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloDataWriter.write(AccumuloDataWriter.scala:13)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:118)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:64)\n",
       "  ... 65 more\n",
       "Caused by: java.lang.IllegalStateException: Can not add to mutation after serializing it\n",
       "  at org.apache.accumulo.core.data.Mutation$Options.put(Mutation.java:1260)\n",
       "  at org.apache.accumulo.core.data.Mutation$Options.put(Mutation.java:1346)\n",
       "  at org.apache.accumulo.core.data.Mutation$Options.put(Mutation.java:1335)\n",
       "  at com.microsoft.ml.spark.accumulo.AccumuloDataWriter$$anonfun$write$1$$anonfun$apply$1.apply(AccumuloDataWriter.scala:36)\n",
       "  at com.microsoft.ml.spark.accumulo.AccumuloDataWriter$$anonfun$write$1$$anonfun$apply$1.apply(AccumuloDataWriter.scala:30)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "  at com.microsoft.ml.spark.accumulo.AccumuloDataWriter$$anonfun$write$1.apply(AccumuloDataWriter.scala:30)\n",
       "  at com.microsoft.ml.spark.accumulo.AccumuloDataWriter$$anonfun$write$1.apply(AccumuloDataWriter.scala:28)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "  at com.microsoft.ml.spark.accumulo.AccumuloDataWriter.write(AccumuloDataWriter.scala:28)\n",
       "  at com.microsoft.ml.spark.accumulo.AccumuloDataWriter.write(AccumuloDataWriter.scala:13)\n",
       "  at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:118)\n",
       "  at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n",
       "  at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)\n",
       "  at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)\n",
       "  at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.write\n",
    "  .format(\"com.microsoft.ml.spark.accumulo\")\n",
    "  .options(properties)\n",
    "  .save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row = [[row_0]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "row_0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val row = df.take(1)(0)\n",
    "row.getStruct(0).getString(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:69: error: missing argument list for method fieldIndex in trait Row\n",
       "Unapplied methods are only converted to functions when a function type is expected.\n",
       "You can make this conversion explicit by writing `fieldIndex _` or `fieldIndex(_)` instead of `fieldIndex`.\n",
       "       row.fieldIndex\n",
       "           ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.fieldIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "// spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accumulo - Scala",
   "language": "scala",
   "name": "accumulo_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
